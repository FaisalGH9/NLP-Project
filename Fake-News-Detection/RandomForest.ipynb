{
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "id": "aIuJtTjKVMXs"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "y7UA9RjoOfhZ"
      },
      "source": [
        "#  Step 1: Import the necessary  library\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "84OBDAQ4Ofhc"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "import re\n",
        "import nltk\n",
        "from nltk.corpus import stopwords\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.naive_bayes import MultinomialNB\n",
        "from sklearn.ensemble import RandomForestClassifier\n",
        "from sklearn.metrics import accuracy_score, classification_report\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "from sklearn.metrics import confusion_matrix\n",
        "from sklearn.metrics import roc_curve, auc\n",
        "import joblib"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "z3X70SxpOfhe"
      },
      "source": [
        "# Step 2: Data Preprocessing"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9vYVTfJaOfhe",
        "outputId": "784ccfb2-eac0-4fba-d4e5-ce963ecbea99"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Dataset Loaded. Shape: (39942, 5)\n",
            "\n",
            "Missing Values:\n",
            " label      0\n",
            "title      0\n",
            "text       0\n",
            "subject    0\n",
            "date       0\n",
            "dtype: int64\n"
          ]
        }
      ],
      "source": [
        "# Load the dataset\n",
        "df = pd.read_csv(\"data.csv\")\n",
        "print(\"Dataset Loaded. Shape:\", df.shape)\n",
        "\n",
        "# Check for missing values\n",
        "print(\"\\nMissing Values:\\n\", df.isnull().sum())"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "id": "4N7OZc6QOfhf"
      },
      "outputs": [],
      "source": [
        "# Text Preprocessing Function\n",
        "def clean_text(text):\n",
        "    text = text.lower()  # Convert to lowercase\n",
        "    text = re.sub(r'\\W', ' ', text)  # Remove punctuation and special characters\n",
        "    text = re.sub(r'\\s+', ' ', text).strip()  # Remove extra spaces\n",
        "    return text"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 33,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "oQoHhFTUOfhg",
        "outputId": "b5c15133-869b-4eb8-80e5-9d384d6352f9"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Sample cleaned text:\n",
            " 0    washington reuters the head of a conservative ...\n",
            "1    washington reuters transgender people will be ...\n",
            "2    washington reuters the special counsel investi...\n",
            "3    washington reuters trump campaign adviser geor...\n",
            "4    seattle washington reuters president donald tr...\n",
            "Name: clean_text, dtype: object\n"
          ]
        }
      ],
      "source": [
        "# Apply text cleaning to the \"text\" column\n",
        "df['clean_text'] = df['text'].map(clean_text)\n",
        "\n",
        "df['clean_title'] = df['title'].map(clean_text)\n",
        "\n",
        "df[\"combined_text\"] = df[\"title\"] + \" \" + df[\"text\"]\n",
        "# Confirm cleaning worked\n",
        "\n",
        "# Confirm cleaning worked\n",
        "print(\"\\nSample cleaned text:\\n\", df['clean_text'].head())\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import nltk\n",
        "nltk.download('stopwords')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-tjF0JWIS1B6",
        "outputId": "a5e443b7-1755-41ad-f21f-03cd8020e49e"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Unzipping corpora/stopwords.zip.\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {},
          "execution_count": 6
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "34p4eiFdOfhg"
      },
      "source": [
        "# Step 3: Model Training"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 36,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "NmSyN-WDOfhh",
        "outputId": "7c14bb9d-24d2-47d7-e9b4-2d5af108d484"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "â±ï¸ Model training execution time: 22.148958 seconds\n"
          ]
        }
      ],
      "source": [
        "start_time = time.time()\n",
        "# Split the dataset (80% Train, 20% Test)\n",
        "X_train, X_test, y_train, y_test = train_test_split(df['combined_text'], df['label'], test_size=0.2, random_state=42)\n",
        "\n",
        "# print(\"Training Set Size:\", len(X_train))\n",
        "# print(\"Testing Set Size:\", len(X_test))\n",
        "\n",
        "custom_stopwords = list(set(stopwords.words('english')))\n",
        "# Apply TF-IDF (Limit to 5000 most important words)\n",
        "tfidf_vectorizer = TfidfVectorizer(max_features=5000, stop_words=custom_stopwords)\n",
        "X_tfidf = tfidf_vectorizer.fit_transform(df['text'])\n",
        "\n",
        "words = np.array(tfidf_vectorizer.get_feature_names_out())\n",
        "# print(\"TF-IDF Shape:\", X_tfidf.shape)\n",
        "\n",
        "\n",
        "# Fit TF-IDF only on training data & transform both train & test\n",
        "X_train_tfidf = tfidf_vectorizer.fit_transform(X_train)\n",
        "X_test_tfidf = tfidf_vectorizer.transform(X_test)\n",
        "\n",
        "# print(\"\\nTF-IDF Applied:\")\n",
        "# print(\"Train Shape:\", X_train_tfidf.shape)\n",
        "# print(\"Test Shape:\", X_test_tfidf.shape)\n",
        "\n",
        "# Convert sparse matrices to dense NumPy arrays\n",
        "X_train_dense = X_train_tfidf.toarray().astype(np.float64)\n",
        "X_test_dense = X_test_tfidf.toarray().astype(np.float64)\n",
        "\n",
        "# Convert labels to NumPy array\n",
        "y_train = np.array(y_train)\n",
        "y_test = np.array(y_test)\n",
        "\n",
        "# Confirm everything is correctly formatted\n",
        "# print(\"\\nData Types & Shapes:\")\n",
        "# print(\"Train Data Type:\", type(X_train_dense))\n",
        "# print(\"Train Data Shape:\", X_train_dense.shape)\n",
        "# print(\"Test Data Type:\", type(X_test_dense))\n",
        "# print(\"Test Data Shape:\", X_test_dense.shape)\n",
        "\n",
        "# End timing\n",
        "end_time = time.time()\n",
        "\n",
        "# Calculate elapsed time\n",
        "elapsed_time = end_time - start_time\n",
        "\n",
        "# Print execution time\n",
        "print(f\"\\nâ±ï¸ Model training execution time: {elapsed_time:.6f} seconds\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "LayZCXHVOfhh",
        "outputId": "984369a4-5d44-4fd5-89f8-fbab1d7ac025"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            " Random Forest Model Trained Successfully!\n"
          ]
        }
      ],
      "source": [
        "\n",
        "# Train the Random Forest Model\n",
        "rf_model = RandomForestClassifier(\n",
        "    n_estimators=100,\n",
        "    max_depth=10,\n",
        "    min_samples_split=10,\n",
        "    random_state=42)\n",
        "rf_model.fit(X_train_dense, y_train)\n",
        "\n",
        "print(\" Random Forest Model Trained Successfully!\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zcjOSJruOfhh"
      },
      "source": [
        "#  Step 4: Evaluate the Model\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "collapsed": true,
        "id": "vJDdbK9TOfhi",
        "outputId": "592a7bab-8cd9-4fb2-e6d2-2f31c5519be9"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            " Random Forest Accuracy: 0.9821\n",
            "\n",
            "ğŸ“Š Classification Report:\n",
            "               precision    recall  f1-score   support\n",
            "\n",
            "           0       0.99      0.97      0.98      3996\n",
            "           1       0.97      0.99      0.98      3993\n",
            "\n",
            "    accuracy                           0.98      7989\n",
            "   macro avg       0.98      0.98      0.98      7989\n",
            "weighted avg       0.98      0.98      0.98      7989\n",
            "\n"
          ]
        }
      ],
      "source": [
        "\n",
        "# Make Predictions\n",
        "y_pred_rf = rf_model.predict(X_test_dense)\n",
        "\n",
        "# Evaluate the Model\n",
        "accuracy_rf = accuracy_score(y_test, y_pred_rf)\n",
        "report_rf = classification_report(y_test, y_pred_rf)\n",
        "\n",
        "print(f\"\\n Random Forest Accuracy: {accuracy_rf:.4f}\")\n",
        "print(\"\\nğŸ“Š Classification Report:\\n\", report_rf)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Get accuracy on training data\n",
        "y_train_pred_rf = rf_model.predict(X_train_dense)\n",
        "train_accuracy_rf = accuracy_score(y_train, y_train_pred_rf)\n",
        "\n",
        "print(f\" Training Accuracy: {train_accuracy_rf:.4f}\")\n",
        "print(f\" Testing Accuracy: {accuracy_rf:.4f}\")  # From previous step\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "PkPztFu5QMxP",
        "outputId": "1d81d243-5342-4107-a031-f47c5715436b"
      },
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            " Training Accuracy: 0.9864\n",
            " Testing Accuracy: 0.9821\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Get feature importance\n",
        "feature_importance = rf_model.feature_importances_\n",
        "\n",
        "# Get top 10 most important words\n",
        "top_n = 10\n",
        "indices = np.argsort(feature_importance)[-top_n:]\n",
        "top_words = [tfidf_vectorizer.get_feature_names_out()[i] for i in indices]\n",
        "\n",
        "print(\"ğŸ”¹ Top 10 Most Important Words for Classification:\")\n",
        "print(top_words)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "d8ub-4pQQs5D",
        "outputId": "0da94411-1df9-41be-899b-6001a0f676ed"
      },
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "ğŸ”¹ Top 10 Most Important Words for Classification:\n",
            "['know', 'video', 'watch', 'getty', 'image', 'washington', 'featured', 'said', 'via', 'reuters']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.model_selection import cross_val_score\n",
        "\n",
        "cv_scores = cross_val_score(rf_model, X_train_dense, y_train, cv=5)\n",
        "print(f\"âœ… Cross-validation Accuracy:,  {cv_scores.mean():.4f} Â± {cv_scores.std():.4f}\")"
      ],
      "metadata": {
        "id": "9GyigvxtQzJf"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Some Graphs present our Results"
      ],
      "metadata": {
        "id": "wQOthOpqlqjX"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Compute confusion matrix\n",
        "cm = confusion_matrix(y_test, y_pred_rf)\n",
        "\n",
        "# Plot confusion matrix\n",
        "plt.figure(figsize=(6,4))\n",
        "sns.heatmap(cm, annot=True, fmt='d', cmap='Blues', xticklabels=['Fake', 'Real'], yticklabels=['Fake', 'Real'])\n",
        "plt.xlabel('Predicted')\n",
        "plt.ylabel('Actual')\n",
        "plt.title('Confusion Matrix - Random Forest')\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "1BuLm-DxvfTM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Get probabilities\n",
        "y_probs_rf = rf_model.predict_proba(X_test_dense)[:,1]\n",
        "\n",
        "# Compute ROC curve\n",
        "fpr, tpr, _ = roc_curve(y_test, y_probs_rf)\n",
        "roc_auc = auc(fpr, tpr)\n",
        "\n",
        "# Plot ROC curve\n",
        "plt.figure(figsize=(6,4))\n",
        "plt.plot(fpr, tpr, label=f'Random Forest (AUC = {roc_auc:.4f})')\n",
        "plt.plot([0, 1], [0, 1], 'k--')  # Diagonal line\n",
        "plt.xlabel('False Positive Rate')\n",
        "plt.ylabel('True Positive Rate')\n",
        "plt.title('ROC Curve')\n",
        "plt.legend()\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "-rL39Y4qmD1_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Get feature importance\n",
        "feature_importance = rf_model.feature_importances_\n",
        "\n",
        "# Get top 10 most important words\n",
        "top_n = 10\n",
        "indices = np.argsort(feature_importance)[-top_n:]\n",
        "top_words = [tfidf_vectorizer.get_feature_names_out()[i] for i in indices]\n",
        "top_values = feature_importance[indices]\n",
        "\n",
        "# Plot feature importance\n",
        "plt.figure(figsize=(8,5))\n",
        "plt.barh(top_words, top_values, color='royalblue')\n",
        "plt.xlabel('Importance Score')\n",
        "plt.ylabel('Top Words')\n",
        "plt.title('Top 10 Most Important Words for Classification')\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "sUPGl8IWmEV4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# comparison between Random Forest and SVM and NaÃ¯ve Bayes"
      ],
      "metadata": {
        "id": "Q0iU5jpjJgFG"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Load trained models\n",
        "nb_model = joblib.load(\"naive_bayes_model.pkl\")\n",
        "rf_model = joblib.load(\"random_forest_model.pkl\")\n",
        "svm_model = joblib.load(\"svm_model.pkl\")"
      ],
      "metadata": {
        "id": "zX7nQMw3Jfpc"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "models = [\"NaÃ¯ve Bayes\", \"Random Forest\", \"SVM\"]\n",
        "training_accuracy = [0.9168, 0.9675, 0.945]  # Replace with actual SVM training accuracy\n",
        "testing_accuracy = [0.9171, 0.9588, 0.937]   # Replace with actual SVM testing accuracy\n",
        "cv_accuracy = [0.9136, 0.9577, 0.934]       # Replace with actual SVM cross-validation accuracy\n",
        "\n",
        "# Bar Width\n",
        "bar_width = 0.25\n",
        "x_indexes = np.arange(len(models))\n",
        "\n",
        "# Create Bar Plot\n",
        "plt.figure(figsize=(10, 6))\n",
        "plt.bar(x_indexes, training_accuracy, width=bar_width, label=\"Training Accuracy\", color='#6a89cc')\n",
        "plt.bar(x_indexes + bar_width, testing_accuracy, width=bar_width, label=\"Testing Accuracy\", color='#82ccdd')\n",
        "plt.bar(x_indexes + 2 * bar_width, cv_accuracy, width=bar_width, label=\"Cross-Validation Accuracy\", color='#b8e994')\n",
        "\n",
        "# Labels & Legends\n",
        "plt.xlabel(\"Models\")\n",
        "plt.ylabel(\"Accuracy\")\n",
        "plt.title(\"Model Accuracy Comparison\")\n",
        "plt.xticks(ticks=x_indexes + bar_width, labels=models)\n",
        "plt.legend()\n",
        "plt.ylim(0.85, 1.0)\n",
        "# Show Plot\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "JFJ_7HuML0y2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Step 5: Predict on validation_data.csv"
      ],
      "metadata": {
        "id": "Ewj01nM5GU3k"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Load and Preprocess the the validation dataset"
      ],
      "metadata": {
        "id": "n1MHKvIbGa3O"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Load the validation dataset\n",
        "df_validation = pd.read_csv(\"validation_data_randomForest.csv\")"
      ],
      "metadata": {
        "id": "0de8uAAOGVUB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Preprocess the text (same cleaning function)\n",
        "df_validation['clean_text'] = df_validation['text'].map(clean_text)\n",
        "\n",
        "# Transform text using the trained TF-IDF vectorizer\n",
        "X_validation_tfidf = tfidf_vectorizer.transform(df_validation['clean_text'])"
      ],
      "metadata": {
        "id": "dXCi2e5wGY05"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Step 6: Predict labels using the trained model"
      ],
      "metadata": {
        "id": "l25QRzV_HPbw"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "df_validation['label'] = rf_model.predict(X_validation_tfidf)\n",
        "# Ensure label \"2\" is replaced with \"0\" or \"1\"\n",
        "df_validation['label'] = df_validation['label'].replace(2, 0)  # Just in case"
      ],
      "metadata": {
        "id": "rWQTZzfYHN52"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Step 7: Save the updated validation file"
      ],
      "metadata": {
        "id": "bQHMMpmYHO2S"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "df_validation.to_csv(\"validation_data_RandomForest.csv\", index=False)\n",
        "\n",
        "print(\" Predictions saved to validation_data_RandomForest.csv\")"
      ],
      "metadata": {
        "id": "1XGfLlwnHcBK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import pickle\n",
        "import time\n",
        "\n",
        "# Ø³Ø¬Ù„ Ø§Ù„ÙˆÙ‚Øª Ù‚Ø¨Ù„ ØªØ­Ù…ÙŠÙ„ Ø§Ù„Ù†Ù…ÙˆØ°Ø¬\n",
        "start_time = time.time()\n",
        "\n",
        "# ØªØ­Ù…ÙŠÙ„ Ø§Ù„Ù†Ù…ÙˆØ°Ø¬ Ù…Ù† Ø§Ù„Ù…Ù„Ù\n",
        "with open(\"random_forest.pkl\", \"rb\") as file:\n",
        "  rf_model = pickle.load(file)\n",
        "\n",
        "# Ø³Ø¬Ù„ Ø§Ù„ÙˆÙ‚Øª Ø¨Ø¹Ø¯ ØªØ­Ù…ÙŠÙ„ Ø§Ù„Ù†Ù…ÙˆØ°Ø¬\n",
        "end_time = time.time()\n",
        "\n",
        "# Ø­Ø³Ø§Ø¨ Ù…Ø¯Ø© Ø§Ù„ØªØ­Ù…ÙŠÙ„\n",
        "loading_time = end_time - start_time\n",
        "print(f\"ğŸ“‚â³ ÙˆÙ‚Øª ØªØ­Ù…ÙŠÙ„ Ø§Ù„Ù†Ù…ÙˆØ°Ø¬ Ù…Ù† `pkl`: {loading_time:.4f} Ø«Ø§Ù†ÙŠØ©\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 211
        },
        "id": "RzPaikqtkvD2",
        "outputId": "95096a9e-68ad-45bc-dab5-4f40dcb8ebfd"
      },
      "execution_count": 20,
      "outputs": [
        {
          "output_type": "error",
          "ename": "UnpicklingError",
          "evalue": "invalid load key, '\\x0c'.",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mUnpicklingError\u001b[0m                           Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-20-32ac34a9f7d6>\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      7\u001b[0m \u001b[0;31m# ØªØ­Ù…ÙŠÙ„ Ø§Ù„Ù†Ù…ÙˆØ°Ø¬ Ù…Ù† Ø§Ù„Ù…Ù„Ù\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      8\u001b[0m \u001b[0;32mwith\u001b[0m \u001b[0mopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"random_forest.pkl\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"rb\"\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mfile\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 9\u001b[0;31m   \u001b[0mrf_model\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpickle\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mload\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfile\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     10\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     11\u001b[0m \u001b[0;31m# Ø³Ø¬Ù„ Ø§Ù„ÙˆÙ‚Øª Ø¨Ø¹Ø¯ ØªØ­Ù…ÙŠÙ„ Ø§Ù„Ù†Ù…ÙˆØ°Ø¬\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mUnpicklingError\u001b[0m: invalid load key, '\\x0c'."
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import pickle\n",
        "\n",
        "# ØªØ­Ù‚Ù‚ Ù…Ù† ÙˆØ¬ÙˆØ¯ Ø§Ù„Ù…Ù„Ù\n",
        "file_path = \"random_forest.pkl\"\n",
        "\n",
        "try:\n",
        "    with open(file_path, \"rb\") as file:\n",
        "        rf_model = pickle.load(file)\n",
        "    print(\"âœ… ØªÙ… ØªØ­Ù…ÙŠÙ„ Ø§Ù„Ù†Ù…ÙˆØ°Ø¬ Ø¨Ù†Ø¬Ø§Ø­\")\n",
        "except Exception as e:\n",
        "    print(f\"âŒ Ø®Ø·Ø£ ÙÙŠ ØªØ­Ù…ÙŠÙ„ Ø§Ù„Ù†Ù…ÙˆØ°Ø¬: {e}\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "GrJFWIpek0rR",
        "outputId": "1da4539a-3a28-4602-f797-10ff6b105108"
      },
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "âŒ Ø®Ø·Ø£ ÙÙŠ ØªØ­Ù…ÙŠÙ„ Ø§Ù„Ù†Ù…ÙˆØ°Ø¬: invalid load key, '\\x0c'.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import joblib\n",
        "\n",
        "# ØªØ­Ù…ÙŠÙ„ Ø§Ù„Ù†Ù…ÙˆØ°Ø¬ Ø¨Ø§Ø³ØªØ®Ø¯Ø§Ù… joblib\n",
        "rf_model = joblib.load(\"random_forest.pkl\")\n",
        "\n",
        "print(\"âœ… ØªÙ… ØªØ­Ù…ÙŠÙ„ Ø§Ù„Ù†Ù…ÙˆØ°Ø¬ Ø¨Ù†Ø¬Ø§Ø­ Ø¨Ø§Ø³ØªØ®Ø¯Ø§Ù… joblib\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "X1FvxpE0ljxW",
        "outputId": "beba7ab9-9fcf-473f-fe2f-f3645e661e02"
      },
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "âœ… ØªÙ… ØªØ­Ù…ÙŠÙ„ Ø§Ù„Ù†Ù…ÙˆØ°Ø¬ Ø¨Ù†Ø¬Ø§Ø­ Ø¨Ø§Ø³ØªØ®Ø¯Ø§Ù… joblib\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "start_time = time.time()\n",
        "\n",
        "# ØªØ­Ù…ÙŠÙ„ Ø§Ù„Ù†Ù…ÙˆØ°Ø¬\n",
        "with open(\"random_forest.pkl\", \"rb\") as file:\n",
        "    rf_model = pickle.load(file)\n",
        "\n",
        "# Ø§Ù†ØªÙ‡Ø§Ø¡ ØªØ³Ø¬ÙŠÙ„ Ø§Ù„ÙˆÙ‚Øª\n",
        "end_time = time.time()\n",
        "\n",
        "# Ø­Ø³Ø§Ø¨ ÙˆÙ‚Øª Ø§Ù„ØªØ­Ù…ÙŠÙ„\n",
        "loading_time = end_time - start_time\n",
        "print(loading_time)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 211
        },
        "id": "6NnXFnF3mD38",
        "outputId": "92ea822e-2d46-4834-dac8-9f0021d49456"
      },
      "execution_count": 26,
      "outputs": [
        {
          "output_type": "error",
          "ename": "UnpicklingError",
          "evalue": "invalid load key, '\\x0c'.",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mUnpicklingError\u001b[0m                           Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-26-f2def8fb0e34>\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;31m# ØªØ­Ù…ÙŠÙ„ Ø§Ù„Ù†Ù…ÙˆØ°Ø¬\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;32mwith\u001b[0m \u001b[0mopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"random_forest.pkl\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"rb\"\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mfile\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 5\u001b[0;31m     \u001b[0mrf_model\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpickle\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mload\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfile\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      6\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      7\u001b[0m \u001b[0;31m# Ø§Ù†ØªÙ‡Ø§Ø¡ ØªØ³Ø¬ÙŠÙ„ Ø§Ù„ÙˆÙ‚Øª\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mUnpicklingError\u001b[0m: invalid load key, '\\x0c'."
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import joblib\n",
        "\n",
        "# ØªØ­Ù…ÙŠÙ„ Ø§Ù„Ù†Ù…ÙˆØ°Ø¬ Ø¨Ø§Ø³ØªØ®Ø¯Ø§Ù… joblib\n",
        "rf_model = joblib.load(\"random_forest.pkl\")\n",
        "\n",
        "print(\"âœ… ØªÙ… ØªØ­Ù…ÙŠÙ„ Ø§Ù„Ù†Ù…ÙˆØ°Ø¬ Ø¨Ù†Ø¬Ø§Ø­ Ø¨Ø§Ø³ØªØ®Ø¯Ø§Ù… joblib\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "pVAaOzS6m82c",
        "outputId": "6f0b3ba4-03f9-4349-89a2-d6a5f6fc1e5a"
      },
      "execution_count": 25,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "âœ… ØªÙ… ØªØ­Ù…ÙŠÙ„ Ø§Ù„Ù†Ù…ÙˆØ°Ø¬ Ø¨Ù†Ø¬Ø§Ø­ Ø¨Ø§Ø³ØªØ®Ø¯Ø§Ù… joblib\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import joblib\n",
        "import time\n",
        "import numpy as np\n",
        "\n",
        "# ØªØ­Ù…ÙŠÙ„ Ø§Ù„Ù†Ù…ÙˆØ°Ø¬\n",
        "rf_model = joblib.load(\"random_forest.pkl\")\n",
        "\n",
        "# ØªÙˆÙ„ÙŠØ¯ Ø¨ÙŠØ§Ù†Ø§Øª Ø§Ø®ØªØ¨Ø§Ø± Ø¹Ø´ÙˆØ§Ø¦ÙŠØ© (Ø§Ø³ØªØ¨Ø¯Ù„Ù‡Ø§ Ø¨Ø¨ÙŠØ§Ù†Ø§ØªÙƒ Ø§Ù„Ø­Ù‚ÙŠÙ‚ÙŠØ©)\n",
        "X_test = np.random.rand(1, rf_model.n_features_in_)  # Ø¹Ø¯Ø¯ Ø§Ù„Ù…ÙŠØ²Ø§Øª ÙŠØ¬Ø¨ Ø£Ù† ÙŠØ·Ø§Ø¨Ù‚ Ù…Ø§ ØªÙ… ØªØ¯Ø±ÙŠØ¨Ù‡\n",
        "\n",
        "# ØªØ³Ø¬ÙŠÙ„ ÙˆÙ‚Øª Ø§Ù„Ø¨Ø¯Ø¡\n",
        "start_time = time.time()\n",
        "\n",
        "# ØªÙ†ÙÙŠØ° Ø§Ù„ØªÙ†Ø¨Ø¤\n",
        "prediction = rf_model.predict(X_test)\n",
        "\n",
        "# ØªØ³Ø¬ÙŠÙ„ ÙˆÙ‚Øª Ø§Ù„Ø§Ù†ØªÙ‡Ø§Ø¡\n",
        "end_time = time.time()\n",
        "\n",
        "# Ø­Ø³Ø§Ø¨ Ø§Ù„Ø²Ù…Ù† Ø§Ù„Ù…Ø³ØªØºØ±Ù‚\n",
        "elapsed_time = end_time - start_time\n",
        "\n",
        "# Ø·Ø¨Ø§Ø¹Ø© Ø§Ù„Ù†ØªØ§Ø¦Ø¬\n",
        "print(f\"âœ… ØªÙ… ØªÙ†ÙÙŠØ° Ø§Ù„ØªÙ†Ø¨Ø¤ Ø¨Ù†Ø¬Ø§Ø­!\")\n",
        "print(f\"â±ï¸ Ø§Ù„Ø²Ù…Ù† Ø§Ù„Ù…Ø³ØªØºØ±Ù‚ Ù„Ù„ØªÙ†Ø¨Ø¤: {elapsed_time:.6f} Ø«Ø§Ù†ÙŠØ©\")\n",
        "print(f\"ğŸ”® Ø§Ù„Ù†ØªÙŠØ¬Ø© Ø§Ù„Ù…ØªÙˆÙ‚Ø¹Ø©: {prediction}\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "CgPL2Zr8nRBM",
        "outputId": "3c973b51-810f-46c0-de26-3899ad31bb71"
      },
      "execution_count": 27,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "âœ… ØªÙ… ØªÙ†ÙÙŠØ° Ø§Ù„ØªÙ†Ø¨Ø¤ Ø¨Ù†Ø¬Ø§Ø­!\n",
            "â±ï¸ Ø§Ù„Ø²Ù…Ù† Ø§Ù„Ù…Ø³ØªØºØ±Ù‚ Ù„Ù„ØªÙ†Ø¨Ø¤: 0.006485 Ø«Ø§Ù†ÙŠØ©\n",
            "ğŸ”® Ø§Ù„Ù†ØªÙŠØ¬Ø© Ø§Ù„Ù…ØªÙˆÙ‚Ø¹Ø©: [1]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "4zPFj92xoBwu"
      },
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.11.5"
    },
    "colab": {
      "provenance": [],
      "machine_shape": "hm",
      "gpuType": "L4"
    },
    "accelerator": "GPU"
  },
  "nbformat": 4,
  "nbformat_minor": 0
}